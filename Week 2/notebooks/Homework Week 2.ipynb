{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 2 Homework\n",
    "\n",
    "In this homework, we'll prepare data for the next week. We'll need\n",
    "to put these datasets to our data lake:\n",
    "\n",
    "* Yellow taxi data for 2019 and 2020. We'll use them for Week #3 lessons\n",
    "* FHV data (for-hire vehicles) for 2019 - for Week #3 homework\n",
    "\n",
    "You can find all the URLs on [the dataset page](https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page)\n",
    "\n",
    "\n",
    "In this homework, we will:\n",
    "\n",
    "* Modify the DAG we created during the lessons for transferring the yellow taxi data\n",
    "* Create a new dag for transferring the FHV data\n",
    "* Create another dag for the Zones data\n",
    "\n",
    "\n",
    "If you don't have access to GCP, you can do that locally and ingest data to Postgres \n",
    "instead. If you have access to GCP, you don't need to do it for local Postgres -\n",
    "only if you want.\n",
    "\n",
    "Also note that for this homework we don't need the last step - creating a table in GCP.\n",
    "After putting all the files to the datalake, we'll create the tables in Week 3.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1: Start date for the Yellow taxi data (1 point)\n",
    "\n",
    "You'll need to parametrize the DAG for processing the yellow taxi data that\n",
    "we created in the videos. \n",
    "\n",
    "What should be the start date for this dag?\n",
    "\n",
    "* 2019-01-01\n",
    "* 2020-01-01\n",
    "* 2021-01-01\n",
    "* days_ago(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2: Frequency for the Yellow taxi data (1 point)\n",
    "\n",
    "How often do we need to run this DAG?\n",
    "\n",
    "* Daily\n",
    "* Monthly\n",
    "* Yearly\n",
    "* Once\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-running the DAGs for past dates\n",
    "\n",
    "To execute your DAG for past dates, try this:\n",
    "\n",
    "* First, delete your DAG from the web interface (the bin icon)\n",
    "* Set the `catchup` parameter to `True`\n",
    "* Be careful with running a lot of jobs in parallel - your system may not like it. Don't set it higher than 3: `max_active_runs=3`\n",
    "* Rename the DAG to something like `data_ingestion_gcs_dag_v02` \n",
    "* Execute it from the Airflow GUI (the play button)\n",
    "\n",
    "\n",
    "Also, there's no data for the recent months, but `curl` will exit successfully.\n",
    "To make it fail on 404, add the `-f` flag:\n",
    "\n",
    "```bash\n",
    "curl -sSLf { URL } > { LOCAL_PATH }\n",
    "```\n",
    "\n",
    "When you run this for all the data, the temporary files will be saved in Docker and will consume your \n",
    "disk space. If it causes problems for you, add another step in your DAG that cleans everything up.\n",
    "It could be a bash operator that runs this command:\n",
    "\n",
    "```bash\n",
    "rm name-of-csv-file.csv name-of-parquet-file.parquet\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3: DAG for FHV Data (2 points)\n",
    "\n",
    "Now create another DAG - for uploading the FHV data. \n",
    "\n",
    "We will need three steps: \n",
    "\n",
    "* Download the data\n",
    "* Parquetize it \n",
    "* Upload to GCS\n",
    "\n",
    "If you don't have a GCP account, for local ingestion you'll need two steps:\n",
    "\n",
    "* Download the data\n",
    "* Ingest to Postgres\n",
    "\n",
    "Use the same frequency and the start date as for the yellow taxi dataset\n",
    "\n",
    "Question: how many DAG runs are green for data in 2019 after finishing everything? \n",
    "\n",
    "Note: when processing the data for 2020-01 you probably will get an error. It's up \n",
    "to you to decide what to do with it - for Week 3 homework we won't need 2020 data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Question 4: DAG for Zones (2 points)\n",
    "\n",
    "\n",
    "Create the final DAG - for Zones:\n",
    "\n",
    "* Download it\n",
    "* Parquetize \n",
    "* Upload to GCS\n",
    "\n",
    "(Or two steps for local ingestion: download -> ingest to postgres)\n",
    "\n",
    "How often does it need to run?\n",
    "\n",
    "* Daily\n",
    "* Monthly\n",
    "* Yearly\n",
    "* Once"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Submitting the solutions\n",
    "\n",
    "* Form for submitting: https://forms.gle/ViWS8pDf2tZD4zSu5\n",
    "* You can submit your homework multiple times. In this case, only the last submission will be used. \n",
    "\n",
    "Deadline: February 7, 17:00 CET "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyarrow import Table\n",
    "from pyarrow.parquet import ParquetWriter\n",
    "src_file = 'H:\\data-engineering-practice\\Week 1\\datasets\\yellow_tripdata_2021-01.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (3862034811.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [2]\u001b[1;36m\u001b[0m\n\u001b[1;33m    for chunk in pd.read_csv('H:\\data-engineering-practice\\Week 1\\datasets\\yellow_tripdata_2021-01.csv',chuunksize=5000):\u001b[0m\n\u001b[1;37m                                                                                                                         ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "for chunk in pd.read_csv(src_file,chunksize=5000):\n",
    "    table = Table.from_pandas(chunk,preserve_index=False)\n",
    "    writer = writer or ParquetWriter(src_file.replace('.csv', '.parquet'))\n",
    "    writer.write_table(table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bb152e7fbe53f8a58c10501dae55bd26da86a96712b1e16de154efa30c81828b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
