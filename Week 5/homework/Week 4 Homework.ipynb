{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78e094b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea557aa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/dexhrestha/spark/spark-3.0.3-bin-hadoop3.2/jars/spark-unsafe_2.12-3.0.3.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "22/02/26 14:25:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/02/26 14:25:35 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName('test') \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77ad4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.po"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c7d66f",
   "metadata": {},
   "source": [
    "## Question 1. Install Spark and PySpark\n",
    "\n",
    "* Install Spark\n",
    "* Run PySpark\n",
    "* Create a local spark session \n",
    "* Execute `spark.version`\n",
    "\n",
    "What's the output?  \n",
    "`Ans: ` 3.0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f420a0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.0.3'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47590b2",
   "metadata": {},
   "source": [
    "\n",
    "## Question 2. HVFHW February 2021\n",
    "\n",
    "Download the HVFHV data for february 2021:\n",
    "\n",
    "```bash\n",
    "wget https://nyc-tlc.s3.amazonaws.com/trip+data/fhvhv_tripdata_2021-02.csv\n",
    "```\n",
    "\n",
    "Read it with Spark using the same schema as we did \n",
    "in the lessons. We will use this dataset for all\n",
    "the remaining questions.\n",
    "\n",
    "Repartition it to 24 partitions and save it to\n",
    "parquet.\n",
    "\n",
    "What's the size of the folder with results (in MB)?  \n",
    "`Ans:` 208M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de84e0a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-02-26 14:26:44--  https://nyc-tlc.s3.amazonaws.com/trip+data/fhvhv_tripdata_2021-02.csv\n",
      "Resolving nyc-tlc.s3.amazonaws.com (nyc-tlc.s3.amazonaws.com)... 52.217.42.220\n",
      "Connecting to nyc-tlc.s3.amazonaws.com (nyc-tlc.s3.amazonaws.com)|52.217.42.220|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 733822658 (700M) [text/csv]\n",
      "Saving to: ‘fhvhv_tripdata_2021-02.csv.1’\n",
      "\n",
      "fhvhv_tripdata_2021 100%[===================>] 699.83M  57.1MB/s    in 11s     \n",
      "\n",
      "2022-02-26 14:26:55 (64.3 MB/s) - ‘fhvhv_tripdata_2021-02.csv.1’ saved [733822658/733822658]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://nyc-tlc.s3.amazonaws.com/trip+data/fhvhv_tripdata_2021-02.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53cb1c7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11613943 fhvhv_tripdata_2021-02.csv\r\n"
     ]
    }
   ],
   "source": [
    "!wc -l fhvhv_tripdata_2021-02.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5eedd3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5019e0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = types.StructType([\n",
    "    types.StructField('hvfhs_license_num', types.StringType(), True),\n",
    "    types.StructField('dispatching_base_num', types.StringType(), True),\n",
    "    types.StructField('pickup_datetime', types.TimestampType(), True),\n",
    "    types.StructField('dropoff_datetime', types.TimestampType(), True),\n",
    "    types.StructField('PULocationID', types.IntegerType(), True),\n",
    "    types.StructField('DOLocationID', types.IntegerType(), True),\n",
    "    types.StructField('SR_Flag', types.StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f4ea8019",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(schema) \\\n",
    "    .csv('fhvhv_tripdata_2021-02.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b811262",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.repartition(24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579faa3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.parquet('fhvhv/2021/02/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf02e433",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read.parquet('fhvhv/2021/02/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "50d18840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- hvfhs_license_num: string (nullable = true)\n",
      " |-- dispatching_base_num: string (nullable = true)\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- dropoff_datetime: timestamp (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- SR_Flag: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "526d2e9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 208M\r\n",
      "-rw-r--r-- 1 dexhrestha dexhrestha    0 Feb 26 14:07 _SUCCESS\r\n",
      "-rw-r--r-- 1 dexhrestha dexhrestha 8.7M Feb 26 14:06 part-00000-d17f6cd5-b801-4ca8-95b4-f5cc6eb06ad7-c000.snappy.parquet\r\n",
      "-rw-r--r-- 1 dexhrestha dexhrestha 8.7M Feb 26 14:06 part-00001-d17f6cd5-b801-4ca8-95b4-f5cc6eb06ad7-c000.snappy.parquet\r\n",
      "-rw-r--r-- 1 dexhrestha dexhrestha 8.7M Feb 26 14:06 part-00002-d17f6cd5-b801-4ca8-95b4-f5cc6eb06ad7-c000.snappy.parquet\r\n",
      "-rw-r--r-- 1 dexhrestha dexhrestha 8.7M Feb 26 14:06 part-00003-d17f6cd5-b801-4ca8-95b4-f5cc6eb06ad7-c000.snappy.parquet\r\n",
      "-rw-r--r-- 1 dexhrestha dexhrestha 8.7M Feb 26 14:06 part-00004-d17f6cd5-b801-4ca8-95b4-f5cc6eb06ad7-c000.snappy.parquet\r\n",
      "-rw-r--r-- 1 dexhrestha dexhrestha 8.7M Feb 26 14:06 part-00005-d17f6cd5-b801-4ca8-95b4-f5cc6eb06ad7-c000.snappy.parquet\r\n",
      "-rw-r--r-- 1 dexhrestha dexhrestha 8.7M Feb 26 14:06 part-00006-d17f6cd5-b801-4ca8-95b4-f5cc6eb06ad7-c000.snappy.parquet\r\n",
      "-rw-r--r-- 1 dexhrestha dexhrestha 8.7M Feb 26 14:06 part-00007-d17f6cd5-b801-4ca8-95b4-f5cc6eb06ad7-c000.snappy.parquet\r\n",
      "-rw-r--r-- 1 dexhrestha dexhrestha 8.7M Feb 26 14:06 part-00008-d17f6cd5-b801-4ca8-95b4-f5cc6eb06ad7-c000.snappy.parquet\r\n",
      "-rw-r--r-- 1 dexhrestha dexhrestha 8.7M Feb 26 14:06 part-00009-d17f6cd5-b801-4ca8-95b4-f5cc6eb06ad7-c000.snappy.parquet\r\n",
      "-rw-r--r-- 1 dexhrestha dexhrestha 8.7M Feb 26 14:06 part-00010-d17f6cd5-b801-4ca8-95b4-f5cc6eb06ad7-c000.snappy.parquet\r\n",
      "-rw-r--r-- 1 dexhrestha dexhrestha 8.7M Feb 26 14:06 part-00011-d17f6cd5-b801-4ca8-95b4-f5cc6eb06ad7-c000.snappy.parquet\r\n",
      "-rw-r--r-- 1 dexhrestha dexhrestha 8.7M Feb 26 14:06 part-00012-d17f6cd5-b801-4ca8-95b4-f5cc6eb06ad7-c000.snappy.parquet\r\n",
      "-rw-r--r-- 1 dexhrestha dexhrestha 8.7M Feb 26 14:06 part-00013-d17f6cd5-b801-4ca8-95b4-f5cc6eb06ad7-c000.snappy.parquet\r\n",
      "-rw-r--r-- 1 dexhrestha dexhrestha 8.7M Feb 26 14:07 part-00014-d17f6cd5-b801-4ca8-95b4-f5cc6eb06ad7-c000.snappy.parquet\r\n",
      "-rw-r--r-- 1 dexhrestha dexhrestha 8.7M Feb 26 14:07 part-00015-d17f6cd5-b801-4ca8-95b4-f5cc6eb06ad7-c000.snappy.parquet\r\n",
      "-rw-r--r-- 1 dexhrestha dexhrestha 8.7M Feb 26 14:07 part-00016-d17f6cd5-b801-4ca8-95b4-f5cc6eb06ad7-c000.snappy.parquet\r\n",
      "-rw-r--r-- 1 dexhrestha dexhrestha 8.7M Feb 26 14:07 part-00017-d17f6cd5-b801-4ca8-95b4-f5cc6eb06ad7-c000.snappy.parquet\r\n",
      "-rw-r--r-- 1 dexhrestha dexhrestha 8.7M Feb 26 14:07 part-00018-d17f6cd5-b801-4ca8-95b4-f5cc6eb06ad7-c000.snappy.parquet\r\n",
      "-rw-r--r-- 1 dexhrestha dexhrestha 8.7M Feb 26 14:07 part-00019-d17f6cd5-b801-4ca8-95b4-f5cc6eb06ad7-c000.snappy.parquet\r\n",
      "-rw-r--r-- 1 dexhrestha dexhrestha 8.7M Feb 26 14:07 part-00020-d17f6cd5-b801-4ca8-95b4-f5cc6eb06ad7-c000.snappy.parquet\r\n",
      "-rw-r--r-- 1 dexhrestha dexhrestha 8.7M Feb 26 14:07 part-00021-d17f6cd5-b801-4ca8-95b4-f5cc6eb06ad7-c000.snappy.parquet\r\n",
      "-rw-r--r-- 1 dexhrestha dexhrestha 8.7M Feb 26 14:07 part-00022-d17f6cd5-b801-4ca8-95b4-f5cc6eb06ad7-c000.snappy.parquet\r\n",
      "-rw-r--r-- 1 dexhrestha dexhrestha 8.7M Feb 26 14:07 part-00023-d17f6cd5-b801-4ca8-95b4-f5cc6eb06ad7-c000.snappy.parquet\r\n"
     ]
    }
   ],
   "source": [
    "ls -lh fhvhv/2021/02"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93033a69",
   "metadata": {},
   "source": [
    "## Question 3. Count records \n",
    "\n",
    "How many taxi trips were there on February 15?  \n",
    "Consider only trips that started on February 15.  \n",
    "`Ans:` 367170"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5f6026be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ec463eca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "367170"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df \\\n",
    "    .withColumn('pickup_date',F.to_date(df.pickup_datetime)) \\\n",
    "    .select('pickup_date','dropoff_datetime','PULocationID','DOLocationID') \\\n",
    "    .where('pickup_date==date(\"2021-02-15\")') \\\n",
    "    .count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151bd88e",
   "metadata": {},
   "source": [
    "\n",
    "## Question 4. Longest trip for each day\n",
    "\n",
    "Now calculate the duration for each trip.\n",
    "\n",
    "Trip starting on which day was the longest?   \n",
    "\n",
    "`Ans:` 2021-02-11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4b99e41e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(hvfhs_license_num='HV0003', dispatching_base_num='B02887', pickup_datetime=datetime.datetime(2021, 2, 6, 1, 18, 35), dropoff_datetime=datetime.datetime(2021, 2, 6, 1, 40, 34), PULocationID=163, DOLocationID=235, SR_Flag=None),\n",
       " Row(hvfhs_license_num='HV0005', dispatching_base_num='B02510', pickup_datetime=datetime.datetime(2021, 2, 5, 7, 13, 6), dropoff_datetime=datetime.datetime(2021, 2, 5, 7, 31, 56), PULocationID=225, DOLocationID=181, SR_Flag=None)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "be7e59c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 169:======================================>                  (2 + 1) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------------+--------+\n",
      "|pickup_date|   dropoff_datetime|duration|\n",
      "+-----------+-------------------+--------+\n",
      "| 2021-02-11|2021-02-12 10:39:44|   75540|\n",
      "+-----------+-------------------+--------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df \\\n",
    "    .withColumn('duration',F.to_timestamp(F.col('dropoff_datetime')).cast('long')-F.to_timestamp(F.col('pickup_datetime')).cast('long')) \\\n",
    "    .selectExpr('cast(pickup_datetime as date) as pickup_date','dropoff_datetime','duration') \\\n",
    "    .sort(F.col('duration'),ascending=False) \\\n",
    "    .show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb6ebff",
   "metadata": {},
   "source": [
    "## Question 5. Most frequent `dispatching_base_num`\n",
    "\n",
    "Now find the most frequently occurring `dispatching_base_num` \n",
    "in this dataset.\n",
    "\n",
    "How many stages this spark job has?  \n",
    "`Ans:` 2 stages\n",
    "> Note: the answer may depend on how you write the query,\n",
    "> so there are multiple correct answers. \n",
    "> Select the one you have.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d937ac7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.registerTempTable('fhv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "271e05b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dispatching_base_count = spark.sql('''\n",
    "    SELECT dispatching_base_num,count(dispatching_base_num) as count\n",
    "    FROM fhv \n",
    "    GROUP BY (dispatching_base_num)\n",
    "    ORDER BY(count) DESC\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "25aeacba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 23:======================================>               (142 + 2) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+\n",
      "|dispatching_base_num|  count|\n",
      "+--------------------+-------+\n",
      "|              B02510|3233664|\n",
      "|              B02764| 965568|\n",
      "|              B02872| 882689|\n",
      "+--------------------+-------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "dispatching_base_count.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "35e6e117",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 49:==========================================>           (158 + 2) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+\n",
      "|dispatching_base_num|  count|\n",
      "+--------------------+-------+\n",
      "|              B02510|3233664|\n",
      "|              B02764| 965568|\n",
      "|              B02872| 882689|\n",
      "+--------------------+-------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df \\\n",
    "    .select('dispatching_base_num') \\\n",
    "    .groupBy('dispatching_base_num') \\\n",
    "    .count().alias('frequency') \\\n",
    "    .orderBy('frequency.count',ascending=False) \\\n",
    "    .show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212c5447",
   "metadata": {},
   "source": [
    "\n",
    "## Question 6. Most common locations pair\n",
    "\n",
    "Find the most common pickup-dropoff pair. \n",
    "\n",
    "For example:\n",
    "\n",
    "\"Jamaica Bay / Clinton East\"\n",
    "\n",
    "Enter two zone names separated by a slash  \n",
    "`Ans:`  East New York / East New York\n",
    "\n",
    "\n",
    "If any of the zone names are unknown (missing), use \"Unknown\". For example, \"Unknown / Clinton East\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "24fb04dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-02-26 14:59:59--  https://s3.amazonaws.com/nyc-tlc/misc/taxi+_zone_lookup.csv\n",
      "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.97.77\n",
      "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.97.77|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 12322 (12K) [application/octet-stream]\n",
      "Saving to: ‘taxi+_zone_lookup.csv.1’\n",
      "\n",
      "taxi+_zone_lookup.c 100%[===================>]  12.03K  --.-KB/s    in 0s      \n",
      "\n",
      "2022-02-26 15:00:00 (89.2 MB/s) - ‘taxi+_zone_lookup.csv.1’ saved [12322/12322]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://s3.amazonaws.com/nyc-tlc/misc/taxi+_zone_lookup.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "733782eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = types.StructType([\n",
    "    types.StructField(\"LocationID\",types.IntegerType(),True),\n",
    "    types.StructField(\"Borough\",types.StringType(),True),\n",
    "    types.StructField(\"Zone\",types.StringType(),True),\n",
    "    types.StructField(\"service_zone\",types.StringType(),True)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "1ccb1e26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+--------------------+------------+\n",
      "|LocationID|Borough|                Zone|service_zone|\n",
      "+----------+-------+--------------------+------------+\n",
      "|         1|    EWR|      Newark Airport|         EWR|\n",
      "|         2| Queens|         Jamaica Bay|   Boro Zone|\n",
      "|         3|  Bronx|Allerton/Pelham G...|   Boro Zone|\n",
      "+----------+-------+--------------------+------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "zones = spark.read.csv('taxi+_zone_lookup.csv',header=True,schema=schema) \n",
    "zones.show(3)          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "6ce5939d",
   "metadata": {},
   "outputs": [],
   "source": [
    "zones.registerTempTable('zones')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "0355f53d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 168:===================================================> (194 + 2) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------+------------+------------+-----+\n",
      "|Zone Pair                    |PULocationID|DOLocationID|count|\n",
      "+-----------------------------+------------+------------+-----+\n",
      "|East New York / East New York|76          |76          |45041|\n",
      "|Borough Park / Borough Park  |26          |26          |37329|\n",
      "|Canarsie / Canarsie          |39          |39          |28026|\n",
      "+-----------------------------+------------+------------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df \\\n",
    "    .groupBy('PULocationID','DOLocationID') \\\n",
    "    .count() \\\n",
    "    .join(zones,(df.PULocationID==zones.LocationID),how='left') \\\n",
    "    .selectExpr('PULocationID','DOLocationID','count','Zone as PUZone') \\\n",
    "    .join(zones,(df.DOLocationID==zones.LocationID),how='left') \\\n",
    "    .selectExpr('PULocationID','DOLocationID','count','PUZone','Zone as DOZone') \\\n",
    "    .withColumn('Zone Pair',F.concat(F.col('PUZone'),F.lit(' / '),F.col('DOZone'))) \\\n",
    "    .select('Zone Pair','PULocationID','DOLocationID','count') \\\n",
    "    .orderBy('count',ascending=False) \\\n",
    "    .show(3,truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8903c43b",
   "metadata": {},
   "source": [
    "## Bonus question. Join type\n",
    "\n",
    "(not graded) \n",
    "\n",
    "For finding the answer to Q6, you'll need to perform a join.\n",
    "\n",
    "What type of join is it?  \n",
    "`Ans:`  The join type is left join  \n",
    "\n",
    "And how many stages your spark job has?  \n",
    "`Ans:`  There are 2 stages"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
